{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from autoimpute.imputations import SingleImputer\n",
    "from autoimpute.imputations import MultipleImputer\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "from aif360.metrics import Metric, DatasetMetric, utils\n",
    "from sklearn import preprocessing\n",
    "from typing import List, Union, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "import adult_loop_script\n",
    "from adult_loop_script import nested_loop\n",
    "from adult_loop_script import nested_loop2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adultdata = pd.read_csv('adult_train.csv')\n",
    "# print(Adultdata.shape[0])\n",
    "# the Adult dataset already has some missing / ? data included. to use a consistent way of creating and analyzing randomness here, we convert the ? values to NA values and remove rows with NA values\n",
    "Adultdata.replace(' ?', np.nan ,inplace =True)\n",
    "Adultdata = Adultdata.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adultdata[\"sex\"] = Adultdata[\"sex\"].astype('category')\n",
    "Adultdata[\"race_class\"] = Adultdata[\"race_class\"].astype('category')\n",
    "Adultdata_Numerical = Adultdata.select_dtypes(exclude = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['race_class', 'sex']\n",
    "\n",
    "data_encoded = Adultdata_Numerical.copy()\n",
    "\n",
    "categorical_names = {}\n",
    "encoders = {}\n",
    "\n",
    "# Use Label Encoder for categorical columns (including target column)\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data_encoded[feature])\n",
    "    \n",
    "    data_encoded[feature] = le.transform(data_encoded[feature])\n",
    "    \n",
    "    categorical_names[feature] = le.classes_\n",
    "    encoders[feature] = le\n",
    "data_perp_sex = data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_race = np.where(categorical_names['race_class'] == 'White')[0]\n",
    "privileged_sex = np.where(categorical_names['sex'] == 'Male')[0]\n",
    "data_standard = StandardDataset(data_perp_sex, \n",
    "                               label_name='income_Class', \n",
    "                               favorable_classes=[1], \n",
    "                               protected_attribute_names=['race_class', 'sex'], \n",
    "                               privileged_classes=[privileged_race, privileged_sex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for calculating the Fairness Metrics\n",
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:                          \n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "# Checking if there exists a dataset with only predictions in the previous condition\n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "# We need to use Classification Metric for calculating 3 Metrics (Equal Oppr, Theil Index & Avg Odds Diff) & BinaryLabel Dataset Metric for the rest        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Function to return fairness Metrics for various Classification Models\n",
    "def get_fair_metrics(data, model, plot=True, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    fair = fair_metrics(data, pred)\n",
    "    return fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested For Loop, Calculating the Fairness Metrics before Imputation on the Original Dataset\n",
    "# We are using three classification Models to calculate the fairness metrics and store the results in a dataframe\n",
    "np.random.seed(123)\n",
    "data_orig_train, data_orig_test = data_standard.split([0.7], shuffle=True)\n",
    "LR_Classifier = LogisticRegression()\n",
    "RF_Classifier = RandomForestClassifier()\n",
    "SV_Classifier = LinearSVC(max_iter=1000)\n",
    "SV_Classifier = CalibratedClassifierCV(SV_Classifier)\n",
    "Classification_Models = [LR_Classifier, RF_Classifier, SV_Classifier]\n",
    "results_pre_names = ['dataset_name', 'num_columns_imputed', 'percentage_deleted', 'imputation_strategy', 'repetition', \n",
    "                    'classification_algorithm', 'accuracy', 'auc', 'F1_score', 'Sensitivity', 'Specificity', \n",
    "                    'Statistical_Parity_Race', 'Statistical_Parity_Sex', 'Equal_Oppr_diff_Race', 'Equal_Oppr_diff_Sex', \n",
    "                    'average_abs_odds_diff_Race', 'average_abs_odds_diff_Sex', 'Disparate_Impact_Race', 'Disparate_Impact_Sex', \n",
    "                    'Theil_Index_Race', 'Theil_Index_Sex']\n",
    "results_pre = pd.DataFrame(columns=results_pre_names)\n",
    "\n",
    "num_repetitions = 100\n",
    "\n",
    "for i in range(num_repetitions):\n",
    "\n",
    "    for classifier in Classification_Models:\n",
    "        log_reg_fit = classifier.fit(data_orig_train.features, \n",
    "                         data_orig_train.labels.ravel(), \n",
    "                         sample_weight=data_orig_train.instance_weights)\n",
    "        X_test = data_orig_test.features\n",
    "        y_test = data_orig_test.labels.ravel()\n",
    "        y_pred = log_reg_fit.predict(X_test)\n",
    "        Accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "        y_pred_proba_final = log_reg_fit.predict_proba(X_test)[:,1]\n",
    "        AUC = roc_auc_score(y_test, y_pred_proba_final)\n",
    "\n",
    "        F1_score = metrics.f1_score(y_test, y_pred)\n",
    "        confusion_matrix_pre = metrics.confusion_matrix(y_test, y_pred)\n",
    "        TP = confusion_matrix_pre[1, 1]\n",
    "        TN = confusion_matrix_pre[0, 0]\n",
    "        FP = confusion_matrix_pre[0, 1]\n",
    "        FN = confusion_matrix_pre[1, 0]\n",
    "        Sensitivity = TP / (TP + FN) \n",
    "        Specificity = TN / (TN + FP)\n",
    "        fair_final = get_fair_metrics(data_orig_test, log_reg_fit)   \n",
    "        Statistical_Parity_Race = fair_final.iloc[1,0]\n",
    "        Statistical_Parity_Sex = fair_final.iloc[2,0]\n",
    "        Equal_Oppr_diff_Race = fair_final.iloc[1,1]\n",
    "        Equal_Oppr_diff_Sex = fair_final.iloc[2,1]\n",
    "        average_abs_odds_diff_Race = fair_final.iloc[1,2]\n",
    "        average_abs_odds_diff_Sex = fair_final.iloc[2,2]\n",
    "        Disparate_Impact_Race = fair_final.iloc[1,3]\n",
    "        Disparate_Impact_Sex = fair_final.iloc[2,3]\n",
    "        Theil_Index_Race = fair_final.iloc[1,4]\n",
    "        Theil_Index_Sex = fair_final.iloc[2,4]\n",
    "        new_row_pre = ['Adult_Pre',0,0,'None',i, classifier.__class__.__name__, Accuracy, AUC, F1_score, Sensitivity, Specificity, Statistical_Parity_Race, \n",
    "                       Statistical_Parity_Sex, Equal_Oppr_diff_Race, Equal_Oppr_diff_Sex, average_abs_odds_diff_Race, average_abs_odds_diff_Sex, \n",
    "                       Disparate_Impact_Race, Disparate_Impact_Sex, Theil_Index_Race, Theil_Index_Sex]\n",
    "        results_pre.loc[len(results_pre)] = new_row_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data_orig_train.feature_names)\n",
    "local_data_train = pd.DataFrame(data= data_orig_train.features, columns= cols)\n",
    "local_data_test = pd.DataFrame(data= data_orig_test.features, columns= cols)\n",
    "local_data_train['income_Class'] = pd.DataFrame(data= data_orig_train.labels)\n",
    "local_data_test['income_Class'] = pd.DataFrame(data= data_orig_test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data_orig_train.feature_names)\n",
    "Protected_Variables = [\"sex\", \"income_Class\", \"race_class\"]\n",
    "cols1 = list(set(cols) - set(Protected_Variables))\n",
    "col2 = local_data_train.columns\n",
    "\n",
    "# Defining all the imputations that we are using on Numerical variables\n",
    "Simple_Mean = SimpleImputer(missing_values= nan, strategy='mean')\n",
    "knn = KNNImputer(n_neighbors=2)\n",
    "#Most_Freq = SimpleImputer(missing_values= nan, strategy=\"most_frequent\" )\n",
    "Simple_Median = SimpleImputer(missing_values= nan, strategy='median')\n",
    "Iterative = IterativeImputer(BayesianRidge())\n",
    "linear_Auto = SingleImputer(strategy=\"interpolate\")\n",
    "lm_ft = SingleImputer(strategy=\"least squares\")\n",
    "stoch_ft = SingleImputer(strategy=\"stochastic\")\n",
    "# pmm_ft = SingleImputer(strategy=\"pmm\")\n",
    "norm_ft = SingleImputer(strategy=\"norm\")\n",
    "\n",
    "imputation_types = [Simple_Mean, knn, Simple_Median, Iterative, linear_Auto, lm_ft, stoch_ft, norm_ft]\n",
    "imputation_type2 = [linear_Auto, lm_ft, stoch_ft, norm_ft]\n",
    "\n",
    "LR_Classifier = LogisticRegression()\n",
    "RF_Classifier = RandomForestClassifier()\n",
    "SV_Classifier = LinearSVC(max_iter=1000)\n",
    "SV_Classifier = CalibratedClassifierCV(SV_Classifier)\n",
    "\n",
    "Classification_Models = [LR_Classifier, RF_Classifier, SV_Classifier]\n",
    "\n",
    "# The percent of data that we need to delete at random\n",
    "percentage_list = [0.05,0.01,0.1] \n",
    "\n",
    "results_df_names = ['dataset_name', 'num_columns_imputed', 'percentage_deleted', 'imputation_strategy', 'repetition', \n",
    "                    'classification_algorithm', 'accuracy', 'auc', 'F1_score', 'Sensitivity', 'Specificity', \n",
    "                    'Statistical_Parity_Race', 'Statistical_Parity_Sex', 'Equal_Oppr_diff_Race', 'Equal_Oppr_diff_Sex', \n",
    "                    'average_abs_odds_diff_Race', 'average_abs_odds_diff_Sex', 'Disparate_Impact_Race', 'Disparate_Impact_Sex', \n",
    "                    'Theil_Index_Race', 'Theil_Index_Sex']\n",
    "results_df = pd.DataFrame(columns=results_df_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypool = multiprocessing.Pool(12)\n",
    "iterable = list(range(1, 101))\n",
    "list_of_results = mypool.map(partial(nested_loop, cols1 = cols1, col2 = col2, local_data_train = local_data_train, \n",
    "                   data_orig_test = data_orig_test, percentage_list = percentage_list, imputation_types = imputation_types, \n",
    "                   imputation_type2 = imputation_type2, Classification_Models = Classification_Models, results_df = results_df, privileged_sex=privileged_sex, privileged_race=privileged_race), iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat(list_of_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the same steps as above for categorical variables\n",
    "Adultdata_Categorical = Adultdata.select_dtypes(exclude = 'int64')\n",
    "Adultdata_Categorical[\"income_Class\"] = Adultdata[\"income_Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race_class', 'sex', 'native-country', 'Age_class']\n",
    "data_encoded = Adultdata_Categorical.copy()\n",
    "\n",
    "categorical_names = {}\n",
    "encoders = {}\n",
    "\n",
    "# Use Label Encoder for categorical columns (including target column)\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data_encoded[feature])\n",
    "    \n",
    "    data_encoded[feature] = le.transform(data_encoded[feature])\n",
    "    \n",
    "    categorical_names[feature] = le.classes_\n",
    "    encoders[feature] = le\n",
    "data_perp_sex = data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standard = StandardDataset(data_perp_sex, \n",
    "                               label_name='income_Class', \n",
    "                               favorable_classes=[1], \n",
    "                               protected_attribute_names=['race_class', 'sex'], \n",
    "                               privileged_classes=[privileged_race, privileged_sex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "data_orig_train, data_orig_test = data_standard.split([0.7], shuffle=True)\n",
    "LR_Classifier = LogisticRegression()\n",
    "RF_Classifier = RandomForestClassifier()\n",
    "SV_Classifier = LinearSVC(max_iter=1000)\n",
    "SV_Classifier = CalibratedClassifierCV(SV_Classifier)\n",
    "Classification_Models = [LR_Classifier, RF_Classifier, SV_Classifier]\n",
    "results_pre_names = ['dataset_name', 'num_columns_imputed', 'percentage_deleted', 'imputation_strategy', 'repetition', \n",
    "                    'classification_algorithm', 'accuracy', 'auc', 'F1_score', 'Sensitivity', 'Specificity', \n",
    "                    'Statistical_Parity_Race', 'Statistical_Parity_Sex', 'Equal_Oppr_diff_Race', 'Equal_Oppr_diff_Sex', \n",
    "                    'average_abs_odds_diff_Race', 'average_abs_odds_diff_Sex', 'Disparate_Impact_Race', 'Disparate_Impact_Sex', \n",
    "                    'Theil_Index_Race', 'Theil_Index_Sex']\n",
    "\n",
    "results_pre_cat = pd.DataFrame(columns=results_pre_names)\n",
    "\n",
    "num_repetitions = 100\n",
    "\n",
    "for i in range(num_repetitions):\n",
    "\n",
    "    for classifier in Classification_Models:\n",
    "        log_reg_fit = classifier.fit(data_orig_train.features, \n",
    "                         data_orig_train.labels.ravel(), \n",
    "                         sample_weight=data_orig_train.instance_weights)\n",
    "        X_test = data_orig_test.features\n",
    "        y_test = data_orig_test.labels.ravel()\n",
    "        y_pred = log_reg_fit.predict(X_test)\n",
    "        Accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "        y_pred_proba_final = log_reg_fit.predict_proba(X_test)[:,1]\n",
    "        AUC = roc_auc_score(y_test, y_pred_proba_final)\n",
    "\n",
    "        F1_score = metrics.f1_score(y_test, y_pred)\n",
    "        confusion_matrix_pre = metrics.confusion_matrix(y_test, y_pred)\n",
    "        TP = confusion_matrix_pre[1, 1]\n",
    "        TN = confusion_matrix_pre[0, 0]\n",
    "        FP = confusion_matrix_pre[0, 1]\n",
    "        FN = confusion_matrix_pre[1, 0]\n",
    "        Sensitivity = TP / (TP + FN) \n",
    "        Specificity = TN / (TN + FP)\n",
    "        fair_final = get_fair_metrics(data_orig_test, log_reg_fit)   \n",
    "        Statistical_Parity_Race = fair_final.iloc[1,0]\n",
    "        Statistical_Parity_Sex = fair_final.iloc[2,0]\n",
    "        Equal_Oppr_diff_Race = fair_final.iloc[1,1]\n",
    "        Equal_Oppr_diff_Sex = fair_final.iloc[2,1]\n",
    "        average_abs_odds_diff_Race = fair_final.iloc[1,2]\n",
    "        average_abs_odds_diff_Sex = fair_final.iloc[2,2]\n",
    "        Disparate_Impact_Race = fair_final.iloc[1,3]\n",
    "        Disparate_Impact_Sex = fair_final.iloc[2,3]\n",
    "        Theil_Index_Race = fair_final.iloc[1,4]\n",
    "        Theil_Index_Sex = fair_final.iloc[2,4]\n",
    "        new_row_pre = ['Adult_Pre_Cat',0,0,'None',i, classifier.__class__.__name__, Accuracy, AUC, F1_score, Sensitivity, Specificity, Statistical_Parity_Race, \n",
    "                       Statistical_Parity_Sex, Equal_Oppr_diff_Race, Equal_Oppr_diff_Sex, average_abs_odds_diff_Race, average_abs_odds_diff_Sex, \n",
    "                       Disparate_Impact_Race, Disparate_Impact_Sex, Theil_Index_Race, Theil_Index_Sex]\n",
    "        results_pre_cat.loc[len(results_pre_cat)] = new_row_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data_orig_train.feature_names)\n",
    "local_data_train = pd.DataFrame(data= data_orig_train.features, columns= cols)\n",
    "local_data_test = pd.DataFrame(data= data_orig_test.features, columns= cols)\n",
    "local_data_train['income_Class'] = pd.DataFrame(data= data_orig_train.labels)\n",
    "local_data_test['income_Class'] = pd.DataFrame(data= data_orig_test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data_orig_train.feature_names)\n",
    "Protected_Variables = [\"sex\", \"income_Class\", \"race_class\"]\n",
    "cols1 = list(set(cols) - set(Protected_Variables))\n",
    "col2 = local_data_train.columns\n",
    "\n",
    "# For categorical variables we use only two types of imputation strategies\n",
    "knn = KNNImputer(n_neighbors=2)\n",
    "Most_Freq = SimpleImputer(missing_values= nan, strategy=\"most_frequent\" )\n",
    "imputation_types = [knn, Most_Freq]\n",
    "\n",
    "LR_Classifier = LogisticRegression()\n",
    "RF_Classifier = RandomForestClassifier()\n",
    "SV_Classifier = LinearSVC(max_iter=1000)\n",
    "SV_Classifier = CalibratedClassifierCV(SV_Classifier)\n",
    "\n",
    "Classification_Models = [LR_Classifier, RF_Classifier, SV_Classifier]\n",
    "\n",
    "percentage_list = [0.05,0.01,0.1]\n",
    "\n",
    "results_df_names = ['dataset_name', 'num_columns_imputed', 'percentage_deleted', 'imputation_strategy', 'repetition', \n",
    "                    'classification_algorithm', 'accuracy', 'auc', 'F1_score', 'Sensitivity', 'Specificity', \n",
    "                    'Statistical_Parity_Race', 'Statistical_Parity_Sex', 'Equal_Oppr_diff_Race', 'Equal_Oppr_diff_Sex', \n",
    "                    'average_abs_odds_diff_Race', 'average_abs_odds_diff_Sex', 'Disparate_Impact_Race', 'Disparate_Impact_Sex', 'Theil_Index_Race', 'Theil_Index_Sex']\n",
    "results_df5 = pd.DataFrame(columns=results_df_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypool = multiprocessing.Pool(10)\n",
    "iterable = list(range(1, 101))\n",
    "list_of_results_categorical = mypool.map(partial(nested_loop2, cols1 = cols1, col2 = col2, local_data_train = local_data_train, \n",
    "                   data_orig_test = data_orig_test, percentage_list = percentage_list, imputation_types = imputation_types, \n",
    "                   imputation_type2 = imputation_type2, Classification_Models = Classification_Models, results_df5 = results_df5, privileged_sex=privileged_sex, privileged_race=privileged_race), iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df5 = pd.concat(list_of_results_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_Pre = pd.concat([results_pre, results_pre_cat])\n",
    "Results_Post = pd.concat([results_df, results_df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_Pre.to_csv('Adult_Pre.csv')\n",
    "Results_Post.to_csv('Adult_Post.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
